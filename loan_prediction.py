# -*- coding: utf-8 -*-
"""Loan_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D4XxoLtOmlsiuZ5nYCP2NxVyHN_PI6Vv
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

data = pd.read_csv('/content/loan_dataset.csv')

data.head()

data.tail()

data.shape

data.info()

data.isnull().sum()

data['Fixed Deposit'].unique()

data['Fixed Deposit'].mode()[0]

pd.crosstab(data['T.Experience'], data['Loan'], margins = True)

data.boxplot(column = 'Income')

data.sample(5)

"""Converting Categorical into Numerical data"""

data['Education'].unique()

data.fillna(0, inplace=True)

label_encoder = LabelEncoder()
data['Education'] = label_encoder.fit_transform(data['Education'])
data['Net Banking'] = label_encoder.fit_transform(data['Net Banking'])
data['Fixed Deposit'] = label_encoder.fit_transform(data['Fixed Deposit'])
data['Demat'] = label_encoder.fit_transform(data['Demat'])
data['Loan'] = label_encoder.fit_transform(data['Loan'])

data.info()

data.describe()

"""Store Feature matrix in X & Response(Target_variable) in Y"""

X = data.drop(columns=['Loan'], axis=1)
y = data['Loan']

print(y)

"""Feature Scaling"""

data.head()

cols = ['ID', 'Pin-code', 'age', 'T.Experience', 'Income', 'Fam members', 'Mortgage']

from sklearn.preprocessing import StandardScaler
st = StandardScaler()
X[cols] = st.fit_transform(X[cols])

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for i in cols:
  X[i] = le.fit_transform(X[i])
X.head()

print(X)

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

model_df={}
def model_val(model, X, y):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)
  # Option 1: Increase max_iter
  # model = LogisticRegression(max_iter=1000)
  # model = LogisticRegression(solver='saga', C=0.1)
  model = LogisticRegression(solver='lbfgs')

# Option 2: Scale the data
  scaler = StandardScaler()
  model = make_pipeline(scaler, LogisticRegression())

# Option 3: Try a different solver
  model = LogisticRegression(solver='saga')
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  print(f"{model} accuracy is {accuracy_score(y_test, y_pred)}")

  score = cross_val_score(model, X, y, cv=5)
  print(f"{model} Avg cross val score is {np.mean(score)}")
  model_df[model] = round(np.mean(score)*100,2)
  print(X_train.shape)
  print(X_test.shape)

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model_val(model, X, y)

print(model_df)

from sklearn import svm
model1 = svm.SVC()
model_val(model1, X, y)

from sklearn.tree import DecisionTreeClassifier
model2 = DecisionTreeClassifier()
model_val(model2, X, y)

from sklearn.ensemble import RandomForestClassifier
model3 = RandomForestClassifier()
model_val(model3, X, y)

from sklearn.ensemble import GradientBoostingClassifier
model4 = GradientBoostingClassifier()
model_val(model4, X, y)

"""HyperParameter Tuning

Logistic Regression
"""

from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV

log_reg_grid = {"C":np.logspace(-4, 4, 20), "solver":['liblinear']}

rs_log_reg = RandomizedSearchCV(LogisticRegression(), param_distributions = log_reg_grid, n_iter = 20, cv = 5, verbose = True)

rs_log_reg.fit(X,y)

rs_log_reg.best_score_

rs_log_reg.best_params_

"""SVC"""

svc_grid = {'C': [0.25, 0.50, 0.75, 1]}  #'kernel': ['linear']

rs_svc = RandomizedSearchCV(svm.SVC(), param_distributions = svc_grid, cv = 5, n_iter = 20, verbose = True)

rs_svc.fit(X,y)

rs_svc.best_score_

rs_svc.best_params_

"""Random Forest Classifer"""

RandomForestClassifier()

rf_grid = {'n_estimators':np.arange(10, 1000, 10), 'max_depth':[None, 3,5, 10, 20, 30], 'min_samples_leaf':[1, 2, 5, 10]}
# min_samples_
# , 'max_features':['auto','sqrt']

rs_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions = rf_grid, cv = 5, n_iter = 20, verbose = True)

rs_rf.fit(X, y)

rs_rf.best_score_

rs_rf.best_params_

"""Save the Best Model"""

X = data.drop(columns=['Loan'], axis=1)
y = data['Loan']

rf = RandomForestClassifier(n_estimators=800, min_samples_leaf=1, max_depth=20)

rf.fit(X, y)

import joblib

joblib.dump(rf, 'Loan_predict')

model = joblib.load('Loan_predict')

print(X)

print(X.iloc[16])

import pandas as pd
df = pd.DataFrame({'Unnamed: 0':9,
                    'ID':9,
                   'Pin-code':0,
                   'age':11,
                   'Fam members':0,
                   'Education':1,
                   'T.Experience':1,
                   'Income':138,
                   'Mortgage': 0,
                   'Fixed Deposit':0,
                   'Demat': 1,
                   'Net Banking':0}
                   ,index=[9])

print(df)

result = model.predict(df)

if(result==1):
  print("Loan Approved")
else:
  print("Loan not Approved")